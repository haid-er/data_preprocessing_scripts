{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9864fbba",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Implements the filling/synchronization plan exactly as specified.\n",
    "\n",
    "Key behaviors:\n",
    " - processes CSVs that DO NOT contain \"_e\" in filename\n",
    " - timestamps are epoch milliseconds (int)\n",
    " - CSVs have no header; columns: timestamp,x,y,z\n",
    " - shift-forward alignment using reference sensors (accelerometer/gyroscope/magnetometer)\n",
    " - append copies of earliest data repeatedly until ~180s (180000 ms)\n",
    " - trim to exactly target (within tolerance)\n",
    " - sort non-monotonic timestamps, fix duplicates (+1 ms)\n",
    " - skip files < 10s, log them\n",
    " - trim files > 180s and log them in Already_OK.txt\n",
    " - write single report.csv with required columns\n",
    " - supports --dry-run to preview without overwriting\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d88d53ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import math\n",
    "import csv\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fd37d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r\"C:\\Users\\Malik Haider\\Documents\\HUMCARE\\DATASET_FILLING\\test_data\"\n",
    "dry_run = False  # or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3d0d566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# USER-CONFIG: update this dictionary with sensor names and their rates (Hz)\n",
    "# Only sensors listed here will be synchronized/processed.\n",
    "# Match is substring-based (case-insensitive) against filenames.\n",
    "sensor_rates = {\n",
    "    \"glass_accelerometer\": 5,\n",
    "    \"glass_gyroscope\": 5,\n",
    "    \"glass_magnetometer\": 5,\n",
    "    \"phone_acceleromter_calibrated\" : 100,\n",
    "    \"phone_accelerometer\": 500,\n",
    "    \"phone_gravity\": 200,\n",
    "    \"phone_linear_acceleration\": 100,\n",
    "    \"phone_gyroscope_uncalibrated\": 500,\n",
    "    \"phone_gyroscope\": 500,\n",
    "    \"phone_magnetometer_uncalibrated\": 100,\n",
    "    \"phone_magnetometer\":  100,\n",
    "    \"phone_interrupt_gyroscope\": 100,\n",
    "    \"watch_accelerometer\": 100,\n",
    "    \"watch_gyroscope_uncalibrated\": 200,\n",
    "    \"watch_linear_acceleration\": 100,\n",
    "    \"watch_gyroscope\": 100,\n",
    "    \"watch_magnetometer_uncalibrated\": 100,\n",
    "    \"watch_magnetometer\": 100,\n",
    "    \"watch_gravity\": 100,\n",
    "}\n",
    "# -------------------------\n",
    "\n",
    "# Constants from the plan\n",
    "TARGET_MS = 170_000\n",
    "MIN_DURATION_MS = 10_000\n",
    "TOLERANCE_MS = 2000  # Â±100 ms acceptable\n",
    "AVG_DELTA_N = 5\n",
    "ROWCOUNT_MISMATCH_THRESHOLD = 0.03  # 3%\n",
    "\n",
    "# Reference categories for computing reference_start\n",
    "REFERENCE_KEYWORDS = (\"accelerometer\", \"gyroscope\", \"magnetometer\")\n",
    "\n",
    "# Log filenames\n",
    "REPORT_FILENAME = \"report.csv\"\n",
    "MIN_LENGTH_FILENAME = \"min_length.txt\"\n",
    "ALREADY_OK_FILENAME = \"Already_OK.txt\"\n",
    "\n",
    "# Report columns\n",
    "REPORT_COLS = [\n",
    "    \"file_path\",\n",
    "    \"sensor_name\",\n",
    "    \"original_rows\",\n",
    "    \"final_rows\",\n",
    "    \"original_start_ms\",\n",
    "    \"original_end_ms\",\n",
    "    \"final_start_ms\",\n",
    "    \"final_end_ms\",\n",
    "    \"original_duration_ms\",\n",
    "    \"final_duration_ms\",\n",
    "    \"sampling_rate_hz\",\n",
    "    \"expected_final_rows\",\n",
    "    \"action_taken\",\n",
    "    \"warnings\",\n",
    "]\n",
    "\n",
    "ALLOWED_ACTIVITIES = [\n",
    "    \"quick_walk\",\n",
    "    \"jogging\",\n",
    "    \"laying\",\n",
    "    \"reading\",\n",
    "    \"sitting\",\n",
    "    \"slow_walk\",\n",
    "    \"standing\",\n",
    "    \"talk_using_phone\",\n",
    "    \"typing\",\n",
    "    \"walking\",\n",
    "    \"clean_the_table\",\n",
    "]  # only process these\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdc721e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_no_header(path):\n",
    "    \"\"\"\n",
    "    Robust reader for CSVs with no header and at least 4 columns:\n",
    "    - reads only first 4 columns (usecols=[0,1,2,3])\n",
    "    - coerces timestamp to numeric, drops bad rows\n",
    "    - converts seconds -> milliseconds if needed\n",
    "    - returns (df, stats) where stats includes counts of dropped rows\n",
    "    \"\"\"\n",
    "    import os\n",
    "    stats = {\n",
    "        \"rows_total\": 0,\n",
    "        \"rows_after_drop_ts\": 0,\n",
    "        \"rows_after_drop_axes\": 0,\n",
    "        \"rows_dropped_invalid_timestamp\": 0,\n",
    "        \"rows_dropped_all_axes\": 0,\n",
    "        \"read_warnings\": \"\"\n",
    "    }\n",
    "    try:\n",
    "        df = pd.read_csv(\n",
    "            path,\n",
    "            header=None,\n",
    "            usecols=[0, 1, 2, 3],\n",
    "            names=[\"timestamp\", \"x\", \"y\", \"z\"],\n",
    "            skipinitialspace=True,\n",
    "            engine=\"python\",\n",
    "            na_values=[\"\", \"NA\", \"nan\", \"NaN\"]\n",
    "        )\n",
    "\n",
    "        df = df.dropna(how=\"all\")\n",
    "        stats[\"rows_total\"] = len(df)\n",
    "        if df.empty:\n",
    "            stats[\"rows_after_drop_ts\"] = 0\n",
    "            stats[\"rows_after_drop_axes\"] = 0\n",
    "            return df.reset_index(drop=True), stats\n",
    "\n",
    "        # Coerce timestamp to numeric (float), drop rows that fail\n",
    "        df[\"timestamp\"] = pd.to_numeric(df[\"timestamp\"], errors=\"coerce\")\n",
    "        before = len(df)\n",
    "        df = df[df[\"timestamp\"].notna()].copy()\n",
    "        stats[\"rows_dropped_invalid_timestamp\"] = before - len(df)\n",
    "        stats[\"rows_after_drop_ts\"] = len(df)\n",
    "\n",
    "        if df.empty:\n",
    "            stats[\"read_warnings\"] = \"all_rows_invalid_timestamps\"\n",
    "            return df.reset_index(drop=True), stats\n",
    "\n",
    "        # Detect seconds vs milliseconds heuristic\n",
    "        max_ts = float(df[\"timestamp\"].max())\n",
    "        if max_ts < 1e11:  # treat as seconds -> convert to ms\n",
    "            df[\"timestamp\"] = df[\"timestamp\"] * 1000.0\n",
    "\n",
    "        df[\"timestamp\"] = df[\"timestamp\"].round().astype(\"int64\")\n",
    "\n",
    "        # Coerce axes to numeric\n",
    "        df[\"x\"] = pd.to_numeric(df[\"x\"], errors=\"coerce\")\n",
    "        df[\"y\"] = pd.to_numeric(df[\"y\"], errors=\"coerce\")\n",
    "        df[\"z\"] = pd.to_numeric(df[\"z\"], errors=\"coerce\")\n",
    "\n",
    "        before_axes = len(df)\n",
    "        # drop rows where all x,y,z are NaN\n",
    "        df = df.dropna(how=\"all\", subset=[\"x\", \"y\", \"z\"])\n",
    "        stats[\"rows_dropped_all_axes\"] = before_axes - len(df)\n",
    "        stats[\"rows_after_drop_axes\"] = len(df)\n",
    "\n",
    "        return df.reset_index(drop=True), stats\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to read {path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "875db8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_activity_folders(base_path):\n",
    "    subjects = [d for d in glob(os.path.join(base_path, \"*\")) if os.path.isdir(d)]\n",
    "    for subj in subjects:\n",
    "        activities = [d for d in glob(os.path.join(subj, \"*\")) if os.path.isdir(d)]\n",
    "        for a in activities:\n",
    "            yield a\n",
    "\n",
    "\n",
    "def find_sensor_files_in_folder(folder, sensor_dict):\n",
    "    \"\"\"\n",
    "    Returns mapping sensor_key -> filepath for sensors present in this folder\n",
    "    matching by substring (case-insensitive).\n",
    "    \"\"\"\n",
    "    files = glob(os.path.join(folder, \"*.csv\"))\n",
    "    lower_to_path = {os.path.basename(fp).lower(): fp for fp in files}\n",
    "    mapping = {}\n",
    "    filename_to_sensor = {}\n",
    "    for fname_lower, path in lower_to_path.items():\n",
    "        if \"_e\" in fname_lower:\n",
    "            continue  # skip files containing _e\n",
    "        for sensor_key in sensor_dict.keys():\n",
    "            if sensor_key.lower() in fname_lower:\n",
    "                mapping[sensor_key] = path\n",
    "                filename_to_sensor[fname_lower] = sensor_key\n",
    "                break\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def sort_and_fix_duplicates(df, actions, warnings):\n",
    "    \"\"\"Sort by timestamp and ensure strictly increasing timestamps by adding small epsilon increments.\"\"\"\n",
    "    need_sort = not df[\"timestamp\"].is_monotonic_increasing\n",
    "    if need_sort:\n",
    "        df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "        actions.append(\"sorted_timestamps\")\n",
    "        warnings.append(\"non_monotonic_sorted\")\n",
    "\n",
    "    # Compute adaptive epsilon based on average delta\n",
    "    avg_delta = compute_avg_delta_ms_from(df)\n",
    "    # epsilon: small fraction of avg_delta but at least 1 ms\n",
    "    epsilon = max(1, int(max(1, round(avg_delta / 10.0))))\n",
    "\n",
    "    ts = df[\"timestamp\"].values.astype(\"int64\")\n",
    "    if len(ts) >= 2:\n",
    "        prev = ts[0]\n",
    "        changed = False\n",
    "        for i in range(1, len(ts)):\n",
    "            if ts[i] <= prev:\n",
    "                ts[i] = prev + epsilon\n",
    "                changed = True\n",
    "            prev = ts[i]\n",
    "        if changed:\n",
    "            df[\"timestamp\"] = ts\n",
    "            actions.append(\"duplicates_fixed\")\n",
    "            warnings.append(f\"duplicates_fixed_epsilon_{epsilon}ms\")\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def compute_avg_delta_ms_from(df, n=AVG_DELTA_N):\n",
    "    \"\"\"\n",
    "    Compute average sampling interval (ms) from the provided dataframe.\n",
    "    Returns integer ms >= 1.\n",
    "    \"\"\"\n",
    "    if df is None or len(df) < 2:\n",
    "        return 1\n",
    "    diffs = df[\"timestamp\"].diff().dropna().astype(\"int64\").values\n",
    "    if len(diffs) == 0:\n",
    "        return 1\n",
    "    n = min(n, len(diffs))\n",
    "    avg = int(diffs[-n:].mean())\n",
    "    return max(1, avg)\n",
    "\n",
    "\n",
    "\n",
    "def shift_forward_timestamps(df, shift_ms):\n",
    "    if shift_ms <= 0:\n",
    "        return df\n",
    "    df = df.copy()\n",
    "    df[\"timestamp\"] = df[\"timestamp\"] + int(shift_ms)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5cdebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_chunks_until_target(df, target_ms, actions, warnings):\n",
    "    \"\"\"\n",
    "    Append repeated copies/partial copies of the earliest data using ORIGINAL file as source.\n",
    "    Uses the original file's average sampling interval to space appended chunks.\n",
    "    \"\"\"\n",
    "    df = df.copy().reset_index(drop=True)\n",
    "    orig_df = df.copy().reset_index(drop=True)\n",
    "    if orig_df.empty:\n",
    "        warnings.append(\"orig_empty_no_append\")\n",
    "        return df\n",
    "\n",
    "    orig_first_ts = int(orig_df[\"timestamp\"].iloc[0])\n",
    "    orig_offsets = (orig_df[\"timestamp\"].astype(\"int64\") - orig_first_ts).astype(\"int64\").values\n",
    "    file_duration = int(orig_offsets[-1]) if len(orig_offsets) > 0 else 0\n",
    "    if file_duration <= 0:\n",
    "        warnings.append(\"zero_file_duration_can_not_append\")\n",
    "        return df\n",
    "\n",
    "    actions.append(\"appended_copied\")\n",
    "\n",
    "    # Use avg delta from the original file to preserve sampling\n",
    "    orig_avg_delta = compute_avg_delta_ms_from(orig_df)\n",
    "\n",
    "    while (int(df[\"timestamp\"].iloc[-1]) - int(df[\"timestamp\"].iloc[0])) < target_ms:\n",
    "        remaining = target_ms - (int(df[\"timestamp\"].iloc[-1]) - int(df[\"timestamp\"].iloc[0]))\n",
    "        chunk_len = file_duration if file_duration <= remaining else remaining\n",
    "\n",
    "        mask = orig_offsets < chunk_len\n",
    "        if not mask.any():\n",
    "            warnings.append(\"no_rows_in_append_window\")\n",
    "            break\n",
    "\n",
    "        chunk = orig_df.loc[mask].copy().reset_index(drop=True)\n",
    "        # base time = current last + orig_avg_delta\n",
    "        base = int(df[\"timestamp\"].iloc[-1]) + int(orig_avg_delta)\n",
    "        prev_ts = int(df[\"timestamp\"].iloc[-1])\n",
    "\n",
    "        chunk_offsets = (chunk[\"timestamp\"].astype(\"int64\") - orig_first_ts).astype(\"int64\").values\n",
    "\n",
    "        proposed_ts = []\n",
    "        for off in chunk_offsets:\n",
    "            t_new = base + int(off)\n",
    "            if t_new <= prev_ts:\n",
    "                # ensure monotonic with small epsilon derived from orig_avg_delta\n",
    "                eps = max(1, int(round(orig_avg_delta / 10.0)))\n",
    "                t_new = prev_ts + eps\n",
    "            proposed_ts.append(t_new)\n",
    "            prev_ts = t_new\n",
    "\n",
    "        chunk[\"timestamp\"] = proposed_ts\n",
    "        df = pd.concat([df, chunk], ignore_index=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e2f755e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_to_target(df, target_ms, actions, warnings):\n",
    "    \"\"\"\n",
    "    Trim rows with timestamp > first_timestamp + target_ms.\n",
    "    Returns trimmed df (may be unchanged) and records action.\n",
    "    \"\"\"\n",
    "    first_ts = int(df[\"timestamp\"].iloc[0])\n",
    "    max_allowed = first_ts + target_ms\n",
    "    if df[\"timestamp\"].iloc[-1] > max_allowed:\n",
    "        df_trimmed = df[df[\"timestamp\"] <= max_allowed].copy().reset_index(drop=True)\n",
    "        actions.append(\"trimmed_to_target\")\n",
    "        return df_trimmed\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def finalize_and_check(df, sensor_name, rate_hz, original_rows, original_start, original_end, actions, warnings, read_stats=None):\n",
    "    \"\"\"\n",
    "    Trim/adjust to ensure final duration equals exactly TARGET_MS (first + TARGET_MS).\n",
    "    Returns df and report_row.\n",
    "    read_stats is an optional dict returned by read_csv_no_header with counts.\n",
    "    \"\"\"\n",
    "    first_ts = int(df[\"timestamp\"].iloc[0])\n",
    "    desired_last = first_ts + TARGET_MS\n",
    "\n",
    "    # If last timestamp is greater than desired, trim rows > desired_last\n",
    "    if int(df[\"timestamp\"].iloc[-1]) > desired_last:\n",
    "        df = df[df[\"timestamp\"] <= desired_last].copy().reset_index(drop=True)\n",
    "        if \"trimmed_to_target\" not in actions:\n",
    "            actions.append(\"trimmed_to_target\")\n",
    "\n",
    "    # Now, ensure last timestamp equals desired_last exactly.\n",
    "    # If last timestamp < desired_last, set last timestamp to desired_last (enforce exact 180s)\n",
    "    last_ts = int(df[\"timestamp\"].iloc[-1])\n",
    "    if last_ts != desired_last:\n",
    "        # Ensure monotonicity: previous timestamp must be < desired_last\n",
    "        if len(df) >= 2:\n",
    "            prev_ts = int(df[\"timestamp\"].iloc[-2])\n",
    "            if prev_ts >= desired_last:\n",
    "                # This is unusual; shift prev_ts downward by epsilon? instead we ensure prev_ts < desired_last by setting it to desired_last - 1\n",
    "                df.at[len(df) - 2, \"timestamp\"] = desired_last - 1\n",
    "            # set final timestamp exactly\n",
    "            df.at[len(df) - 1, \"timestamp\"] = desired_last\n",
    "        else:\n",
    "            # only one row: set its timestamp to first (should already be equal) but enforce last=desired_last by adding one synthetic row?\n",
    "            # We'll set the single row's timestamp to first_ts (can't create new rows here). To guarantee 180s, append a copy of this row with timestamp=desired_last.\n",
    "            row = df.iloc[0].copy()\n",
    "            row[\"timestamp\"] = desired_last\n",
    "            df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "        actions.append(\"enforce_exact_target_last_ts\")\n",
    "\n",
    "    # Recompute final stats\n",
    "    final_rows = len(df)\n",
    "    final_start = int(df[\"timestamp\"].iloc[0])\n",
    "    final_end = int(df[\"timestamp\"].iloc[-1])\n",
    "    final_duration = final_end - final_start\n",
    "\n",
    "    # estimated original rate\n",
    "    original_estimated_rate = None\n",
    "    if original_end is not None and original_start is not None and (original_end - original_start) > 0:\n",
    "        original_estimated_rate = original_rows / ((original_end - original_start) / 1000.0)\n",
    "\n",
    "    expected_final_rows = round(rate_hz * (TARGET_MS / 1000.0)) if rate_hz else None\n",
    "\n",
    "    if rate_hz and expected_final_rows:\n",
    "        diff_pct = abs(final_rows - expected_final_rows) / float(expected_final_rows)\n",
    "        # if diff_pct > ROWCOUNT_MISMATCH_THRESHOLD:\n",
    "        #     print('')\n",
    "            # warnings.append(f\"rowcount_mismatch_{diff_pct:.3f}\")\n",
    "            \n",
    "\n",
    "    action_taken = \",\".join(actions) if actions else \"none\"\n",
    "    warnings_str = \";\".join(warnings) if warnings else \"NA\"\n",
    "\n",
    "    report_row = {\n",
    "        \"file_path\": None,  # filled by caller\n",
    "        \"sensor_name\": sensor_name,\n",
    "        \"original_rows\": original_rows,\n",
    "        \"final_rows\": final_rows,\n",
    "        \"original_start_ms\": original_start,\n",
    "        \"original_end_ms\": original_end,\n",
    "        \"final_start_ms\": final_start,\n",
    "        \"final_end_ms\": final_end,\n",
    "        \"original_duration_ms\": original_end - original_start if original_end and original_start else \"NA\",\n",
    "        \"final_duration_ms\": final_duration,\n",
    "        \"sampling_rate_hz\": rate_hz if rate_hz else \"NA\",\n",
    "        \"expected_final_rows\": expected_final_rows if expected_final_rows is not None else \"NA\",\n",
    "        \"action_taken\": action_taken,\n",
    "        \"warnings\": warnings_str,\n",
    "        # new debug fields\n",
    "        \"rows_dropped_invalid_timestamp\": read_stats.get(\"rows_dropped_invalid_timestamp\", 0) if read_stats else 0,\n",
    "        \"rows_dropped_all_axes\": read_stats.get(\"rows_dropped_all_axes\", 0) if read_stats else 0,\n",
    "        \"original_estimated_rate_hz\": round(original_estimated_rate, 2) if original_estimated_rate else \"NA\",\n",
    "        \"read_warnings\": read_stats.get(\"read_warnings\", \"\") if read_stats else \"\"\n",
    "    }\n",
    "    return df, report_row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b3524e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_activity_folder(folder, sensor_rates_map, dry_run, report_rows, min_length_paths, already_ok_paths):\n",
    "    # Find relevant sensor files in folder (skip _e)\n",
    "    present_mapping = find_sensor_files_in_folder(folder, sensor_rates_map)\n",
    "    # sensors expected list\n",
    "    expected_sensors = list(sensor_rates_map.keys())\n",
    "\n",
    "    # Determine missing sensors and log them (they will be added to report)\n",
    "    for s in expected_sensors:\n",
    "        if s not in present_mapping:\n",
    "            report_rows.append({\n",
    "                \"file_path\": os.path.join(folder, f\"**missing**/{s}\"),\n",
    "                \"sensor_name\": s,\n",
    "                \"original_rows\": 0,\n",
    "                \"final_rows\": 0,\n",
    "                \"original_start_ms\": \"NA\",\n",
    "                \"original_end_ms\": \"NA\",\n",
    "                \"final_start_ms\": \"NA\",\n",
    "                \"final_end_ms\": \"NA\",\n",
    "                \"original_duration_ms\": \"NA\",\n",
    "                \"final_duration_ms\": \"NA\",\n",
    "                \"sampling_rate_hz\": sensor_rates_map.get(s),\n",
    "                \"expected_final_rows\": round(sensor_rates_map[s] * 180) if sensor_rates_map.get(s) else \"NA\",\n",
    "                \"action_taken\": \"missing_sensor\",\n",
    "                \"warnings\": \"missing_sensor\"\n",
    "            })\n",
    "\n",
    "    # Build present sensors mapping: sensor -> path\n",
    "    # For synchronization we will compute reference_start using reference categories\n",
    "    present_sensors = {k: v for k, v in present_mapping.items() if k in sensor_rates_map}\n",
    "\n",
    "    if not present_sensors:\n",
    "        # nothing to do in this folder\n",
    "        return\n",
    "\n",
    "    # Compute reference_start using reference categories among present sensors\n",
    "    reference_candidates = []\n",
    "    for sensor_key, path in present_sensors.items():\n",
    "        lname = os.path.basename(path).lower()\n",
    "        if any(k in sensor_key.lower() for k in REFERENCE_KEYWORDS) or any(kw in lname for kw in REFERENCE_KEYWORDS):\n",
    "            try:\n",
    "                df_tmp, read_stats = read_csv_no_header(path)\n",
    "                if df_tmp.empty:\n",
    "                    continue\n",
    "                reference_candidates.append(int(df_tmp[\"timestamp\"].iloc[0]))\n",
    "            except Exception:\n",
    "                continue\n",
    "    if not reference_candidates:\n",
    "        # fallback to the starts of all present sensors\n",
    "        for sensor_key, path in present_sensors.items():\n",
    "            try:\n",
    "                df_tmp, read_stats = read_csv_no_header(path)\n",
    "                if df_tmp.empty:\n",
    "                    continue\n",
    "                reference_candidates.append(int(df_tmp[\"timestamp\"].iloc[0]))\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    if not reference_candidates:\n",
    "        # can't compute reference start; skip\n",
    "        return\n",
    "\n",
    "    reference_start = max(reference_candidates)\n",
    "\n",
    "    # Now process each present sensor file\n",
    "    for sensor_name, filepath in present_sensors.items():\n",
    "        actions = []\n",
    "        warnings = []\n",
    "        rate_hz = sensor_rates_map.get(sensor_name)\n",
    "        try:\n",
    "            df, read_stats = read_csv_no_header(filepath)\n",
    "        except Exception as e:\n",
    "            # log read failure\n",
    "            report_rows.append({\n",
    "                \"file_path\": filepath,\n",
    "                \"sensor_name\": sensor_name,\n",
    "                \"original_rows\": 0,\n",
    "                \"final_rows\": 0,\n",
    "                \"original_start_ms\": \"NA\",\n",
    "                \"original_end_ms\": \"NA\",\n",
    "                \"final_start_ms\": \"NA\",\n",
    "                \"final_end_ms\": \"NA\",\n",
    "                \"original_duration_ms\": \"NA\",\n",
    "                \"final_duration_ms\": \"NA\",\n",
    "                \"sampling_rate_hz\": rate_hz if rate_hz else \"NA\",\n",
    "                \"expected_final_rows\": round(rate_hz * 180) if rate_hz else \"NA\",\n",
    "                \"action_taken\": \"read_failed\",\n",
    "                \"warnings\": str(e)\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        if df.empty:\n",
    "            # nothing to do\n",
    "            report_rows.append({\n",
    "                \"file_path\": filepath,\n",
    "                \"sensor_name\": sensor_name,\n",
    "                \"original_rows\": 0,\n",
    "                \"final_rows\": 0,\n",
    "                \"original_start_ms\": \"NA\",\n",
    "                \"original_end_ms\": \"NA\",\n",
    "                \"final_start_ms\": \"NA\",\n",
    "                \"final_end_ms\": \"NA\",\n",
    "                \"original_duration_ms\": \"NA\",\n",
    "                \"final_duration_ms\": \"NA\",\n",
    "                \"sampling_rate_hz\": rate_hz if rate_hz else \"NA\",\n",
    "                \"expected_final_rows\": round(rate_hz * 180) if rate_hz else \"NA\",\n",
    "                \"action_taken\": \"empty_file\",\n",
    "                \"warnings\": \"empty_file\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Sort & fix duplicates BEFORE computing original stats\n",
    "        df = sort_and_fix_duplicates(df, actions, warnings)\n",
    "\n",
    "        original_rows = len(df)\n",
    "        original_start = int(df[\"timestamp\"].iloc[0])\n",
    "        original_end = int(df[\"timestamp\"].iloc[-1])\n",
    "        original_duration = original_end - original_start\n",
    "\n",
    "        # Minimum duration check\n",
    "        if original_duration < MIN_DURATION_MS:\n",
    "            # skip and log in min_length.txt\n",
    "            min_length_paths.append((filepath, original_duration))\n",
    "            report_rows.append({\n",
    "                \"file_path\": filepath,\n",
    "                \"sensor_name\": sensor_name,\n",
    "                \"original_rows\": original_rows,\n",
    "                \"final_rows\": 0,\n",
    "                \"original_start_ms\": original_start,\n",
    "                \"original_end_ms\": original_end,\n",
    "                \"final_start_ms\": \"NA\",\n",
    "                \"final_end_ms\": \"NA\",\n",
    "                \"original_duration_ms\": original_duration,\n",
    "                \"final_duration_ms\": \"NA\",\n",
    "                \"sampling_rate_hz\": rate_hz if rate_hz else \"NA\",\n",
    "                \"expected_final_rows\": round(rate_hz * 180) if rate_hz else \"NA\",\n",
    "                \"action_taken\": \"skipped_too_short\",\n",
    "                \"warnings\": \"skipped_too_short\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Shift-forward to align start to reference_start\n",
    "        shift_ms = reference_start - original_start\n",
    "        if shift_ms > 0:\n",
    "            df = shift_forward_timestamps(df, shift_ms)\n",
    "            actions.append(\"shifted_start\")\n",
    "            # Update original_start/end to shifted values for reporting continuity\n",
    "            original_start = int(df[\"timestamp\"].iloc[0])\n",
    "            original_end = int(df[\"timestamp\"].iloc[-1])\n",
    "\n",
    "        # Recompute duration after shift\n",
    "        current_duration = int(df[\"timestamp\"].iloc[-1]) - int(df[\"timestamp\"].iloc[0])\n",
    "\n",
    "        # If file originally longer than TARGET_MS (we trim and log)\n",
    "        if current_duration > TARGET_MS:\n",
    "            # Trim to target\n",
    "            df = trim_to_target(df, TARGET_MS, actions, warnings)\n",
    "            already_ok_paths.append((filepath, current_duration))\n",
    "            # finalize and save later\n",
    "            df, report = finalize_and_check(df, sensor_name, rate_hz, original_rows, original_start, original_end, actions, warnings)\n",
    "            report[\"file_path\"] = filepath\n",
    "            report_rows.append(report)\n",
    "            if not dry_run:\n",
    "                # overwrite file\n",
    "                df.to_csv(filepath, index=False, header=False, float_format=\"%.6f\")\n",
    "            else:\n",
    "                print(f\"[dry-run] would trim (longer) and overwrite: {filepath}\")\n",
    "            continue\n",
    "\n",
    "        # If file shorter than target -> append repeated earliest data\n",
    "        if current_duration < TARGET_MS:\n",
    "            df_appended = append_chunks_until_target(df, TARGET_MS, actions, warnings)\n",
    "            # then trim if overshot\n",
    "            df_appended = trim_to_target(df_appended, TARGET_MS, actions, warnings)\n",
    "            # finalize checks\n",
    "            df_final, report = finalize_and_check(df_appended, sensor_name, rate_hz, original_rows, original_start, original_end, actions, warnings)\n",
    "            report[\"file_path\"] = filepath\n",
    "            report_rows.append(report)\n",
    "            if not dry_run:\n",
    "                df_final.to_csv(filepath, index=False, header=False, float_format=\"%.6f\")\n",
    "            else:\n",
    "                print(f\"[dry-run] would append/trim and overwrite: {filepath}\")\n",
    "            continue\n",
    "\n",
    "        # If current_duration == TARGET_MS (already exact) -> still finalize (no changes)\n",
    "        df, report = finalize_and_check(df, sensor_name, rate_hz, original_rows, original_start, original_end, actions, warnings)\n",
    "        report[\"file_path\"] = filepath\n",
    "        report_rows.append(report)\n",
    "        if not dry_run:\n",
    "            df.to_csv(filepath, index=False, header=False, float_format=\"%.6f\")\n",
    "        else:\n",
    "            print(f\"[dry-run] file already exactly target: {filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23ba53aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_logs(base_path, report_rows, min_length_paths, already_ok_paths):\n",
    "    # Write report.csv\n",
    "    report_path = os.path.join(base_path, REPORT_FILENAME)\n",
    "    with open(report_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=REPORT_COLS)\n",
    "        writer.writeheader()\n",
    "        for r in report_rows:\n",
    "            # ensure all keys present\n",
    "            row = {k: r.get(k, \"NA\") for k in REPORT_COLS}\n",
    "            writer.writerow(row)\n",
    "\n",
    "    # Write min_length.txt\n",
    "    min_path = os.path.join(base_path, MIN_LENGTH_FILENAME)\n",
    "    with open(min_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for p, dur in min_length_paths:\n",
    "            f.write(f\"{p},{dur}\\n\")\n",
    "\n",
    "    # Write Already_OK.txt\n",
    "    ak_path = os.path.join(base_path, ALREADY_OK_FILENAME)\n",
    "    with open(ak_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for p, dur in already_ok_paths:\n",
    "            f.write(f\"{p},{dur}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cd24b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7af1c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a3632fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def driver(base_path, dry_run=False):\n",
    "    if not os.path.isdir(base_path):\n",
    "        print(f\"Base path not found: {base_path}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    report_rows = []\n",
    "    min_length_paths = []\n",
    "    already_ok_paths = []\n",
    "\n",
    "    activity_folders = list_activity_folders(base_path)\n",
    "    for folder in activity_folders:\n",
    "        activity_name = os.path.basename(folder).lower()\n",
    "        if ALLOWED_ACTIVITIES and activity_name not in ALLOWED_ACTIVITIES:\n",
    "            continue  # skip this activity\n",
    "        try:\n",
    "            process_activity_folder(folder, sensor_rates, dry_run, report_rows, min_length_paths, already_ok_paths)\n",
    "        except Exception as e:\n",
    "            # log folder-level failure as a report row\n",
    "            report_rows.append({\n",
    "                \"file_path\": folder,\n",
    "                \"sensor_name\": \"folder_error\",\n",
    "                \"original_rows\": \"NA\",\n",
    "                \"final_rows\": \"NA\",\n",
    "                \"original_start_ms\": \"NA\",\n",
    "                \"original_end_ms\": \"NA\",\n",
    "                \"final_start_ms\": \"NA\",\n",
    "                \"final_end_ms\": \"NA\",\n",
    "                \"original_duration_ms\": \"NA\",\n",
    "                \"final_duration_ms\": \"NA\",\n",
    "                \"sampling_rate_hz\": \"NA\",\n",
    "                \"expected_final_rows\": \"NA\",\n",
    "                \"action_taken\": \"folder_processing_failed\",\n",
    "                \"warnings\": str(e)\n",
    "            })\n",
    "\n",
    "    # Write logs (even in dry-run, per your plan we still output report files)\n",
    "    write_logs(base_path, report_rows, min_length_paths, already_ok_paths)\n",
    "\n",
    "    print(\"Processing complete.\")\n",
    "    print(f\"Report written to: {os.path.join(base_path, REPORT_FILENAME)}\")\n",
    "    print(f\"Min length list: {os.path.join(base_path, MIN_LENGTH_FILENAME)}\")\n",
    "    print(f\"Already long / trimmed: {os.path.join(base_path, ALREADY_OK_FILENAME)}\")\n",
    "    if dry_run:\n",
    "        print(\"Dry-run mode: no files were overwritten.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50131a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Processing complete.\n",
      "Report written to: C:\\Users\\Malik Haider\\Documents\\HUMCARE\\DATASET_FILLING\\test_data\\report.csv\n",
      "Min length list: C:\\Users\\Malik Haider\\Documents\\HUMCARE\\DATASET_FILLING\\test_data\\min_length.txt\n",
      "Already long / trimmed: C:\\Users\\Malik Haider\\Documents\\HUMCARE\\DATASET_FILLING\\test_data\\Already_OK.txt\n"
     ]
    }
   ],
   "source": [
    "driver(base_path, dry_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599b680f-1225-4ef0-8445-ca8cd1b14100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8f7c67-73ed-4100-9b39-11285bcc2baa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
